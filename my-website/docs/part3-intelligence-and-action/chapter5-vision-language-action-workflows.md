# Chapter 5: Vision-Language-Action Workflows

This chapter will delve into connecting language models to robotic actions. This is a rapidly advancing area of research that is making it possible to control robots using natural language.

## Connecting Language Models to Robotic Actions

The core idea behind Vision-Language-Action (VLA) models is to connect a large language model (LLM) to a robot's sensors and actuators. The LLM can then be used to:

*   **Understand natural language commands**: For example, "pick up the red block" or "go to the kitchen".
*   **Reason about the world**: The LLM can use its knowledge of the world to make decisions about how to best accomplish a task.
*   **Generate robot actions**: The LLM can generate a sequence of actions that will cause the robot to perform the desired task.

## LLM-driven Task Planning for Robots

LLMs can also be used for high-level task planning. For example, you could give an LLM a high-level goal, such as "clean the kitchen", and it could generate a sequence of sub-tasks, such as:

1.  Pick up the trash.
2.  Wipe the counters.
3.  Wash the dishes.

The LLM could then generate the low-level actions required to complete each of these sub-tasks.

## Real-world Application Examples

VLA models are already being used in a variety of real-world applications, such as:

*   **Warehouse automation**: Robots are using VLA models to pick and place items in warehouses.
*   **Healthcare**: Robots are using VLA models to assist patients with daily tasks.
*   **Manufacturing**: Robots are using VLA models to assemble products.

In this chapter, we will explore how to build a simple VLA model for our humanoid robot and use it to perform a variety of tasks.
